#!/usr/bin/env python
"""
train.py - 
Model generator for langid.py
Marco Lui November 2011

Based on research by Marco Lui and Tim Baldwin.

Copyright 2011 Marco Lui <saffsd@gmail.com>. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are
permitted provided that the following conditions are met:

   1. Redistributions of source code must retain the above copyright notice, this list of
      conditions and the following disclaimer.

   2. Redistributions in binary form must reproduce the above copyright notice, this list
      of conditions and the following disclaimer in the documentation and/or other materials
      provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER ``AS IS'' AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

The views and conclusions contained in the software and documentation are those of the
authors and should not be interpreted as representing official policies, either expressed
or implied, of the copyright holder.
"""

import numpy as np
import base64
import bz2
import array
import cPickle
from collections import deque

class Scanner(object):
  alphabet = map(chr, range(1<<8))
  """
  Implementation of Aho-Corasick string matching.
  This class should be instantiated with a set of keywords, which
  will then be the only tokens generated by the class's search method,
  """
  def __init__(self, keywords):
    self.build(keywords)

  def __call__(self, value):
    return self.search(value)

  def build(self, keywords):
    goto = dict()
    fail = dict()
    output = defaultdict(set)

    # Algorithm 2
    newstate = 0
    for a in keywords:
      state = 0
      j = 0
      while (j < len(a)) and (state, a[j]) in goto:
        state = goto[(state, a[j])]
        j += 1
      for p in range(j, len(a)):
        newstate += 1
        goto[(state, a[p])] = newstate
        #print "(%d, %s) -> %d" % (state, a[p], newstate)
        state = newstate
      output[state].add(a)
    for a in self.alphabet:
      if (0,a) not in goto: 
        goto[(0,a)] = 0

    # Algorithm 3
    queue = deque()
    for a in self.alphabet:
      if goto[(0,a)] != 0:
        s = goto[(0,a)]
        queue.append(s)
        fail[s] = 0
    while queue:
      r = queue.popleft()
      for a in self.alphabet:
        if (r,a) in goto:
          s = goto[(r,a)]
          queue.append(s)
          state = fail[r]
          while (state,a) not in goto:
            state = fail[state]
          fail[s] = goto[(state,a)]
          #print "f(%d) -> %d" % (s, goto[(state,a)]), output[fail[s]]
          if output[fail[s]]:
            output[s].update(output[fail[s]])

    # Algorithm 4
    self.nextmove = {}
    for a in self.alphabet:
      self.nextmove[(0,a)] = goto[(0,a)]
      if goto[(0,a)] != 0:
        queue.append(goto[(0,a)])
    while queue:
      r = queue.popleft()
      for a in self.alphabet:
        if (r,a) in goto:
          s = goto[(r,a)]
          queue.append(s)
          self.nextmove[(r,a)] = s
        else:
          self.nextmove[(r,a)] = self.nextmove[(fail[r],a)]

    # convert the output to tuples, as tuple iteration is faster
    # than set iteration
    self.output = dict((k, tuple(output[k])) for k in output)

    # Next move encoded as a single array. The index of the next state
    # is located at current state * alphabet size  + ord(c).
    # The choice of 'H' array typecode limits us to 64k states.
    def nextstate_iter():
      for state in xrange(len(set(self.nextmove.values()))):
        for letter in self.alphabet:
          yield self.nextmove[(state, letter)]
    self.nm_arr = array.array('H', nextstate_iter())

  def __getstate__(self):
    """
    Compiled nextmove and output.
    """
    return (self.nm_arr, self.output)

  def __setstate__(self, value):
    nm_array, output = value
    self.nm_arr = nm_array
    self.output = output
    self.nextmove = {}
    for i, next_state in enumerate(nm_array):
      state = i / 256
      letter = chr(i % 256)
      self.nextmove[(state, letter)] = next_state 

  def search(self, string):
    # TODO: update to using nm_arr
    state = 0
    for letter in string:
      state = self.nextmove[(state, letter)]
      for key in self.output.get(state, []):
        yield key

  def tokenize(self, text):
    c = Counter()
    state = 0
    for letter in map(ord,text):
      state = self.nm_arr[(state << 8) + letter]
      c.update( self.output.get(state, []) )
    return c

def set_nmarr(arg):
  """
  Set the global next-move array used by the aho-corasick scanner
  """
  global nm_arr
  nm_arr = arg

from collections import defaultdict
def pass2(path):
  """
  Returns counts of how often each state was entered
  """
  global nm_arr
  c = defaultdict(int)
  state = 0
  with open(path) as f:
    text = f.read()
    for letter in map(ord,text):
      state = nm_arr[(state << 8) + letter]
      c[state] += 1
  return c


def nb_learn(fm, cm):
  tot_cl = cm.shape[1]
  #used_cl = np.flatnonzero(cm.sum(0) > 0)
  #cm = sp.csr_matrix(cm[:,used_cl], dtype='int32')
  v = fm.shape[1]
  prod = np.dot(fm.T, cm)
  pc = np.log(cm.sum(0))
  ptc = np.log(1 + prod) - np.log(v + prod.sum(0))
  return pc, ptc

def index(seq):
  return dict((k,v) for (v,k) in enumerate(seq))







import sys
import os
import multiprocessing as mp
import optparse
if __name__ == "__main__":
  parser = optparse.OptionParser()
  parser.add_option("-o","--output", dest="outfile", help="output model to FILE", metavar="FILE")
  parser.add_option("-c","--corpus", dest="corpus", help="read corpus from DIR", metavar="DIR")
  parser.add_option("-i","--input", dest="infile", help="read features from FILE", metavar="FILE")
  parser.add_option("-j","--jobs", dest="job_count", type="int", help="number of processes to use", default=mp.cpu_count())
  options, args = parser.parse_args()
  
  print "target dir: ", options.corpus 
  langs = set()
  paths = []
  for dirpath, dirnames, filenames in os.walk(options.corpus):
    for f in filenames:
      paths.append(os.path.join(dirpath, f))
      langs.add(os.path.basename(dirpath))
  print "found %d files" % len(paths)
  print "langs(%d): %s" % (len(langs), sorted(langs))

  nb_features = map(eval, open(options.infile))
  feat_index = index(nb_features)

  # Build the actual scanner
  print "building scanner"
  scanner = Scanner(nb_features)
  tk_nextmove, raw_output = scanner.__getstate__()

  # Map the scanner raw output directly into feature indexes
  state2feat = {}
  for k,v in raw_output.items():
    state2feat[k] = tuple(feat_index[f] for f in v)
    
  nm_arr = mp.Array('i', tk_nextmove, lock=False)
  pool = mp.Pool(options.job_count, set_nmarr, (nm_arr,))
  print "spawned pool of %d processes" % options.job_count

  num_instances = len(paths)
  num_classes = len(langs)
  num_features = len(nb_features)

  # Generate the class map
  lang_index = index(sorted(langs))
  cm = np.zeros((num_instances, num_classes), dtype='bool')
  for docid, path in enumerate(paths):
    lang = os.path.basename(os.path.dirname(path))
    cm[docid, lang_index[lang]] = True
  print "generated class map"
  
  # Generate the feature map
  def get_paths():
    for i,p in enumerate(paths):
      if i % 100 == 0:
        print "%d..." % i
      yield p
  fm = np.zeros((num_instances, num_features), dtype='int')
  output_states = set(state2feat)
  statesets = pool.imap(pass2, get_paths())
  for docid, stateset in enumerate(statesets):
    # Process each stateset
    for state in (set(stateset) & output_states):
      for f_id in state2feat[state]:
        fm[docid, f_id] += stateset[state]

  print "generated feature map"

  pc, ptc = nb_learn(fm, cm)
  nb_classes = sorted(lang_index, key=lang_index.get)
  nb_pc = array.array('d', pc)
  nb_ptc = array.array('d')
  for term_dist in ptc.tolist():
    nb_ptc.extend(term_dist)

  # tk_output is the output function of the scanner. It should generate indices into
  # the feature space directly, as this saves a lookup
  tk_output = {}
  for key in raw_output:
    tk_output[key] = tuple(feat_index[v] for v in raw_output[key])
  
  model = nb_ptc, nb_pc, nb_classes, tk_nextmove, tk_output
  string = base64.b64encode(bz2.compress(cPickle.dumps(model)))
  with open(options.outfile, 'w') as f:
    f.write(string)
