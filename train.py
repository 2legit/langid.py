#!/usr/bin/env python
"""
train.py - 
Model generator for langid.py
Marco Lui November 2011

Based on research by Marco Lui and Tim Baldwin.

Copyright 2011 Marco Lui <saffsd@gmail.com>. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are
permitted provided that the following conditions are met:

   1. Redistributions of source code must retain the above copyright notice, this list of
      conditions and the following disclaimer.

   2. Redistributions in binary form must reproduce the above copyright notice, this list
      of conditions and the following disclaimer in the documentation and/or other materials
      provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER ``AS IS'' AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

The views and conclusions contained in the software and documentation are those of the
authors and should not be interpreted as representing official policies, either expressed
or implied, of the copyright holder.
"""

import base64, bz2, cPickle
import os, sys, optparse
import array
import numpy as np
import multiprocessing as mp
from collections import deque, defaultdict
from contextlib import closing

class Scanner(object):
  alphabet = map(chr, range(1<<8))
  """
  Implementation of Aho-Corasick string matching.
  This class should be instantiated with a set of keywords, which
  will then be the only tokens generated by the class's search method,
  """
  def __init__(self, keywords):
    self.build(keywords)

  def __call__(self, value):
    return self.search(value)

  def build(self, keywords):
    goto = dict()
    fail = dict()
    output = defaultdict(set)

    # Algorithm 2
    newstate = 0
    for a in keywords:
      state = 0
      j = 0
      while (j < len(a)) and (state, a[j]) in goto:
        state = goto[(state, a[j])]
        j += 1
      for p in range(j, len(a)):
        newstate += 1
        goto[(state, a[p])] = newstate
        #print "(%d, %s) -> %d" % (state, a[p], newstate)
        state = newstate
      output[state].add(a)
    for a in self.alphabet:
      if (0,a) not in goto: 
        goto[(0,a)] = 0

    # Algorithm 3
    queue = deque()
    for a in self.alphabet:
      if goto[(0,a)] != 0:
        s = goto[(0,a)]
        queue.append(s)
        fail[s] = 0
    while queue:
      r = queue.popleft()
      for a in self.alphabet:
        if (r,a) in goto:
          s = goto[(r,a)]
          queue.append(s)
          state = fail[r]
          while (state,a) not in goto:
            state = fail[state]
          fail[s] = goto[(state,a)]
          #print "f(%d) -> %d" % (s, goto[(state,a)]), output[fail[s]]
          if output[fail[s]]:
            output[s].update(output[fail[s]])

    # Algorithm 4
    self.nextmove = {}
    for a in self.alphabet:
      self.nextmove[(0,a)] = goto[(0,a)]
      if goto[(0,a)] != 0:
        queue.append(goto[(0,a)])
    while queue:
      r = queue.popleft()
      for a in self.alphabet:
        if (r,a) in goto:
          s = goto[(r,a)]
          queue.append(s)
          self.nextmove[(r,a)] = s
        else:
          self.nextmove[(r,a)] = self.nextmove[(fail[r],a)]

    # convert the output to tuples, as tuple iteration is faster
    # than set iteration
    self.output = dict((k, tuple(output[k])) for k in output)

    # Next move encoded as a single array. The index of the next state
    # is located at current state * alphabet size  + ord(c).
    # The choice of 'H' array typecode limits us to 64k states.
    def nextstate_iter():
      for state in xrange(len(set(self.nextmove.values()))):
        for letter in self.alphabet:
          yield self.nextmove[(state, letter)]
    self.nm_arr = array.array('H', nextstate_iter())

  def __getstate__(self):
    """
    Compiled nextmove and output.
    """
    return (self.nm_arr, self.output)

  def __setstate__(self, value):
    nm_array, output = value
    self.nm_arr = nm_array
    self.output = output
    self.nextmove = {}
    for i, next_state in enumerate(nm_array):
      state = i / 256
      letter = chr(i % 256)
      self.nextmove[(state, letter)] = next_state 

  def search(self, string):
    # TODO: update to using nm_arr
    state = 0
    for letter in string:
      state = self.nextmove[(state, letter)]
      for key in self.output.get(state, []):
        yield key

def chunk(seq, chunksize):
  """
  Break a sequence into chunks not exceeeding a predetermined size
  """
  seq_iter = iter(seq)
  while True:
    chunk = tuple(seq_iter.next() for i in range(chunksize))
    if len(chunk) == 0:
      break
    yield chunk

def unmarshal_iter(f):
  """
  Iterator over a file object, which unmarshals
  item by item.
  """
  while True:
    try:
      yield marshal.load(f)
    except EOFError:
      break

def index(seq):
  """
  Build an index for a sequence of items. Assumes
  that the items in the sequence are unique.
  @param seq the sequence to index
  @returns a dictionary from item to position in the sequence
  """
  return dict((k,v) for (v,k) in enumerate(seq))


def set_nmarr(arg):
  """
  Set the global next-move array used by the aho-corasick scanner
  """
  global nm_arr
  nm_arr = arg

def pass2(path):
  """
  Returns counts of how often each state was entered
  """
  global nm_arr
  c = defaultdict(int)
  state = 0
  with open(path) as f:
    text = f.read()
    for letter in map(ord,text):
      state = nm_arr[(state << 8) + letter]
      c[state] += 1
  return c


def learn_pc(cm):
  """
  @param cm class map
  @returns nb_pc: log(P(C))
  """
  pc = np.log(cm.sum(0))
  nb_pc = array.array('d', pc)
  return nb_pc

def learn_ptc(fm, cm):
  """
  Learn naive bayes parameters from a feature map and 
  a class map. We use the multinomial event model.
  @param fm feature map
  @param cm class map
  @returns nb_ptc: log(P(t|C))
  """
  tot_cl = cm.shape[1]
  v = fm.shape[1]
  prod = np.dot(fm.T, cm)
  ptc = np.log(1 + prod) - np.log(v + prod.sum(0))
  nb_ptc = array.array('d')
  for term_dist in ptc.tolist():
    nb_ptc.extend(term_dist)
  return nb_ptc

def generate_cm(paths, langs):
  num_instances = len(paths)
  num_classes = len(langs)

  # Generate the class map
  lang_index = index(sorted(langs))
  cm = np.zeros((num_instances, num_classes), dtype='bool')
  for docid, path in enumerate(paths):
    lang = os.path.basename(os.path.dirname(path))
    cm[docid, lang_index[lang]] = True
  nb_classes = sorted(lang_index, key=lang_index.get)
  print "generated class map"

  return nb_classes, cm

def generate_fm(paths, nb_features, tk_nextmove, state2feat):
  num_instances = len(paths)
  num_features = len(nb_features)

  # Generate the feature map
  fm = np.zeros((num_instances, num_features), dtype='int')
  output_states = set(state2feat)
  nm_arr = mp.Array('i', tk_nextmove, lock=False)
  with closing( mp.Pool(options.job_count, set_nmarr, (nm_arr,)) 
              ) as pool:
    statesets = pool.imap(pass2, paths)

  for docid, stateset in enumerate(statesets):
    # Process each stateset
    for state in (set(stateset) & output_states):
      for f_id in state2feat[state]:
        fm[docid, f_id] += stateset[state]
  print "generated feature map"
  return fm

def read_corpus(path):
  print "data directory: ", path
  langs = set()
  paths = []
  for dirpath, dirnames, filenames in os.walk(path):
    for f in filenames:
      paths.append(os.path.join(dirpath, f))
      langs.add(os.path.basename(dirpath))
  print "found %d files" % len(paths)
  print "langs(%d): %s" % (len(langs), sorted(langs))
  return paths, langs

def build_scanner(nb_features):
  feat_index = index(nb_features)

  # Build the actual scanner
  print "building scanner"
  scanner = Scanner(nb_features)
  tk_nextmove, raw_output = scanner.__getstate__()

  # tk_output is the output function of the scanner. It should generate indices into
  # the feature space directly, as this saves a lookup
  tk_output = {}
  for key in raw_output:
    tk_output[key] = tuple(feat_index[v] for v in raw_output[key])
  
  # Map the scanner raw output directly into feature indexes
  state2feat = {}
  for k,v in raw_output.items():
    state2feat[k] = tuple(feat_index[f] for f in v)
  return tk_nextmove, tk_output, state2feat

if __name__ == "__main__":
  parser = optparse.OptionParser()
  parser.add_option("-o","--output", dest="outfile", help="output model to FILE", metavar="FILE")
  parser.add_option("-c","--corpus", dest="corpus", help="read corpus from DIR", metavar="DIR")
  parser.add_option("-i","--input", dest="infile", help="read features from FILE", metavar="FILE")
  parser.add_option("-j","--jobs", dest="job_count", type="int", help="number of processes to use", default=mp.cpu_count())
  options, args = parser.parse_args()
  
  paths, langs = read_corpus(options.corpus)
  nb_features = map(eval, open(options.infile))
  nb_classes, cm = generate_cm(paths, langs)
  tk_nextmove, tk_output, state2feat = build_scanner(nb_features)
  fm = generate_fm(paths, nb_features, tk_nextmove, state2feat)
  nb_pc = learn_pc(cm)
  nb_ptc = learn_ptc(fm, cm)

  # output the model
  model = nb_ptc, nb_pc, nb_classes, tk_nextmove, tk_output
  string = base64.b64encode(bz2.compress(cPickle.dumps(model)))
  with open(options.outfile, 'w') as f:
    f.write(string)
  print "wrote model to %s (%d bytes)" % (options.outfile, len(string))
